{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dcac814",
   "metadata": {},
   "source": [
    "# Pr√°ctica 1 - Red Neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5ab944",
   "metadata": {},
   "source": [
    "### Utilidades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecfe126",
   "metadata": {},
   "source": [
    "#### Configuraci√≥n del dispositivo de c√≥mputo\n",
    "Selecciona autom√°ticamente GPU si est√° disponible, o CPU en caso contrario, e imprime el dispositivo usado para entrenar la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0fb26117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def device():\n",
    "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e869ebd",
   "metadata": {},
   "source": [
    "#### Fijaci√≥n de semilla para reproducibilidad\n",
    "La siguiente funci√≥n, `seed_everything`, asegura que los experimentos sean reproducibles fijando la semilla para PyTorch, CUDA, NumPy y el generador de n√∫meros aleatorios de Python. Tambi√©n configura CUDA para un comportamiento determinista cuando se usa GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e2cecf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4347e588",
   "metadata": {},
   "source": [
    "#### An√°lisis de clases y manejo de desbalanceo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d63438",
   "metadata": {},
   "source": [
    "##### Obtener distribuci√≥n de clases\n",
    "Esta funci√≥n carga un dataset organizado con `ImageFolder`, extrae todas las etiquetas y devuelve la distribuci√≥n de im√°genes por clase. Para ello usa `Counter` para contar cu√°ntas veces aparece cada etiqueta y obtiene la lista de nombres de clases desde el propio dataset. No aplica transformaciones complejas porque solo necesita los labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d0921ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from collections import Counter\n",
    "\n",
    "def get_dataset_class_distribution(dataset_path_train):\n",
    "    \"\"\"\n",
    "    Carga un dataset con ImageFolder y devuelve:\n",
    "    - distribuci√≥n por clase (Counter)\n",
    "    - lista de nombres de clases\n",
    "    \"\"\"\n",
    "        \n",
    "    # Transform simple (solo para obtener labels)\n",
    "    dummy_transform = transforms.ToTensor()\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(root=dataset_path_train, transform=dummy_transform)\n",
    "\n",
    "    dist = Counter([label for _, label in train_dataset])\n",
    "\n",
    "    class_names = train_dataset.classes\n",
    "\n",
    "    return dist, class_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abe35b3",
   "metadata": {},
   "source": [
    "##### Calcular pesos por clase\n",
    "Esta funci√≥n recibe el n√∫mero de muestras por clase, calcula pesos inversos para compensar clases minoritarias, normaliza los pesos y retorna un array y un tensor listo para usar en `CrossEntropyLoss`. Es una t√©cnica est√°ndar para entrenar clasificadores en datasets desbalanceados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "43f276ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def compute_class_weights(class_counts, normalize=True):\n",
    "    \"\"\"\n",
    "    Recibe un array con el n√∫mero de muestras por clase.\n",
    "    Devuelve:\n",
    "      - class_weights (numpy array)\n",
    "      - class_weights_tensor (tensor listo para CrossEntropyLoss)\n",
    "    \"\"\"\n",
    "\n",
    "    DEVICE = device()\n",
    "    class_counts = np.array(class_counts, dtype=float)\n",
    "\n",
    "    # Pesos inversos: menos muestras ‚Üí m√°s peso\n",
    "    class_weights = 1.0 / class_counts\n",
    "\n",
    "    if normalize:\n",
    "        class_weights = class_weights / class_weights.sum()\n",
    "\n",
    "    # Convertir a tensor\n",
    "    class_weights_tensor = torch.FloatTensor(class_weights).to(DEVICE)\n",
    "\n",
    "    return class_weights, class_weights_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8639f9",
   "metadata": {},
   "source": [
    "##### Construir un WeightedRandomSampler\n",
    "Genera un sampler que selecciona muestras con una probabilidad proporcional al peso de su clase. Esto permite sobre-representar clases minoritarias durante el entrenamiento sin duplicar datos ni modificar el dataset. Es una t√©cnica fundamental para entrenar modelos con datasets desbalanceados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3ea6a3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "import torch\n",
    "\n",
    "def create_weighted_sampler(train_dataset, class_weights):\n",
    "    \"\"\"\n",
    "    Recibe:\n",
    "      - train_dataset (ImageFolder)\n",
    "      - class_weights (numpy array)\n",
    "    Devuelve un WeightedRandomSampler listo para usar en DataLoader.\n",
    "    \"\"\"\n",
    "\n",
    "    # Peso de cada muestra seg√∫n su clase\n",
    "    sample_weights = [class_weights[label] for _, label in train_dataset]\n",
    "    sample_weights = torch.DoubleTensor(sample_weights)\n",
    "\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1673c55f",
   "metadata": {},
   "source": [
    "#### Preparaci√≥n de datasets y dataloaders sin Data Augmentation\n",
    "\n",
    "La funci√≥n `prepare_dataloaders` se encarga de cargar las im√°genes del conjunto de datos a partir de la estructura de carpetas (train/, val/, test/) y aplicar las transformaciones necesarias antes del entrenamiento. \n",
    "\n",
    "Seg√∫n el n√∫mero de canales indicado, convierte las im√°genes a escala de grises o las mantiene en color, las redimensiona y las transforma en tensores. \n",
    "\n",
    "Despu√©s, crea los dataloaders correspondientes para entrenamiento, validaci√≥n y prueba, configurados con `batch size`, `shuffle` y carga paralela mediante `num_workers`. \n",
    "\n",
    "Finalmente, devuelve los tres dataloaders junto con la lista de clases detectadas en el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "eaf12047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def prepare_dataloaders(dataset_path: str,\n",
    "                        batch_size=64,\n",
    "                        img_size=(48, 48),\n",
    "                        input_channels=1,\n",
    "                        num_workers=2):\n",
    "    \n",
    "    if input_channels == 1:\n",
    "        transform_list = [\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.Resize(img_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5]) \n",
    "            \n",
    "        ]\n",
    "    else:\n",
    "        transform_list = [\n",
    "            transforms.Resize(img_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Imagenet stats\n",
    "        ]\n",
    "        \n",
    "    data_transform = transforms.Compose(transform_list)\n",
    "\n",
    "    train_dir = os.path.join(dataset_path, 'train')\n",
    "    val_dir = os.path.join(dataset_path, 'val') \n",
    "    test_dir = os.path.join(dataset_path, 'test')\n",
    "\n",
    "    # Cargar Datasets\n",
    "    train_dataset = datasets.ImageFolder(root=train_dir, transform=data_transform)\n",
    "    val_dataset = datasets.ImageFolder(root=val_dir, transform=data_transform)\n",
    "    test_dataset = datasets.ImageFolder(root=test_dir, transform=data_transform)\n",
    "\n",
    "    # Dataloaders\n",
    "    dist, class_names = get_dataset_class_distribution(train_dir)\n",
    "    class_counts = [dist[i] for i in range(len(class_names))]  \n",
    "    class_weights, class_weights_tensor = compute_class_weights(class_counts)\n",
    "    sampler = create_weighted_sampler(train_dataset, class_weights)\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, train_dataset.classes, class_weights_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370630db",
   "metadata": {},
   "source": [
    "#### Preparaci√≥n de datasets y dataloaders con Data Augmentation\n",
    "\n",
    "**`get_train_transforms()`**  \n",
    "Define y construye el conjunto de **transformaciones aplicadas durante el entrenamiento**.  \n",
    "Incluye:\n",
    "- Conversi√≥n a escala de grises (1 o 3 canales seg√∫n lo requerido por el modelo).\n",
    "- T√©cnicas de *data augmentation* (recorte aleatorio, rotaci√≥n, traslaci√≥n, blur, etc.).\n",
    "- Conversi√≥n a tensor y borrado aleatorio (*Random Erasing*).\n",
    "\n",
    "Estas transformaciones aumentan la variabilidad de los datos, reducen el sobreajuste y mejoran la capacidad de generalizaci√≥n del modelo.\n",
    "\n",
    "---\n",
    "\n",
    "**`get_eval_transforms()`**  \n",
    "Define las transformaciones aplicadas en **validaci√≥n y test**.  \n",
    "En este caso **no se utiliza data augmentation**, solo:\n",
    "- Conversi√≥n a escala de grises.\n",
    "- Redimensionamiento al tama√±o objetivo.\n",
    "- Conversi√≥n a tensor.\n",
    "\n",
    "El objetivo es mantener las im√°genes de evaluaci√≥n consistentes y medir el rendimiento del modelo sin alteraciones aleatorias.\n",
    "\n",
    "---\n",
    "\n",
    "**`prepare_dataloaders_data_augmentation()`**  \n",
    "Funci√≥n principal encargada de:\n",
    "1. Aplicar las transformaciones correspondientes seg√∫n el tipo de conjunto (entrenamiento o evaluaci√≥n).  \n",
    "2. Cargar las im√°genes desde las carpetas `train/`, `val/` y `test`.  \n",
    "3. Crear los **DataLoader** de PyTorch para cada conjunto:  \n",
    "   - `train_loader` (con *shuffle*)  \n",
    "   - `val_loader`  \n",
    "   - `test_loader`  \n",
    "4. Devolver tambi√©n la lista de clases del dataset.\n",
    "\n",
    "Esta funci√≥n proporciona un pipeline modular y limpio para preparar los datos, compatible con cualquier modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fcf3227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCIONES DE PREPARACI√ìN DE DATOS CON DATA AUGMENTATION\n",
    "\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# TRANSFORMACIONES DE ENTRENAMIENTO\n",
    "def get_train_transforms(img_size=(48, 48), input_channels=1):\n",
    "    \n",
    "    transform_list = []\n",
    "\n",
    "    # Convertir canales\n",
    "    if input_channels  == 1:\n",
    "        transform_list.append(transforms.Grayscale(num_output_channels=1))\n",
    "        mean = [0.5]\n",
    "        std = [0.5]\n",
    "    elif input_channels == 3:\n",
    "        # Si las im√°genes son grises pero quieres 3 canales para ResNet\n",
    "        transform_list.append(transforms.Grayscale(num_output_channels=3))\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "    else:\n",
    "        raise ValueError(\"output_channels debe ser 1 o 3.\")\n",
    "\n",
    "    # Data augmentation\n",
    "    transform_list += [\n",
    "        transforms.RandomResizedCrop(img_size, scale=(0.9, 1.0)), # Peque√±o recorte aleatorio y redimensionamiento\n",
    "        transforms.RandomHorizontalFlip(p=0.5), # Volteo horizontal aleatorio\n",
    "        transforms.RandomRotation(10), # Rotaci√≥n aleatoria de ¬±10 grados\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "    ]\n",
    "\n",
    "    return transforms.Compose(transform_list)\n",
    "\n",
    "\n",
    "# TRANSFORMACIONES DE VALIDACI√ìN/TEST\n",
    "def get_eval_transforms(img_size=(48, 48), input_channels=1):\n",
    "\n",
    "    transform_list = []\n",
    "\n",
    "    if input_channels == 1:\n",
    "        transform_list.append(transforms.Grayscale(num_output_channels=1))\n",
    "        mean=[0.5]\n",
    "        std=[0.5]\n",
    "    elif input_channels == 3:\n",
    "        transform_list.append(transforms.Grayscale(num_output_channels=3))\n",
    "        mean=[0.485, 0.456, 0.406]\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "\n",
    "    transform_list += [\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ]\n",
    "\n",
    "    return transforms.Compose(transform_list)\n",
    "\n",
    "\n",
    "# FUNCI√ìN PRINCIPAL: DATALOADERS\n",
    "def prepare_dataloaders_data_augmentation(\n",
    "        dataset_path: str,\n",
    "        batch_size=64,\n",
    "        img_size=(48, 48),\n",
    "        input_channels=1,\n",
    "        num_workers=2):\n",
    "\n",
    "    train_transform = get_train_transforms(img_size, input_channels)\n",
    "    eval_transform  = get_eval_transforms(img_size, input_channels)\n",
    "\n",
    "    train_dir = os.path.join(dataset_path, 'train')\n",
    "    val_dir   = os.path.join(dataset_path, 'val')\n",
    "    test_dir  = os.path.join(dataset_path, 'test')\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
    "    val_dataset   = datasets.ImageFolder(root=val_dir, transform=eval_transform)\n",
    "    test_dataset  = datasets.ImageFolder(root=test_dir, transform=eval_transform)\n",
    "\n",
    "    # Dataloaders\n",
    "    dist, class_names = get_dataset_class_distribution(train_dir)\n",
    "    class_counts = [dist[i] for i in range(len(class_names))]  \n",
    "    class_weights, class_weights_tensor = compute_class_weights(class_counts)\n",
    "    \n",
    "    sampler = create_weighted_sampler(train_dataset, class_weights)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=num_workers, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, train_dataset.classes, class_weights_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee832bae",
   "metadata": {},
   "source": [
    "#### Gr√°fico de las curvas de accuracy durante el entrenamiento y la validaci√≥n de la red neuronal.\n",
    "A continuaci√≥n, se definir√° una funci√≥n que muestre c√≥mo cambian la p√©rdida (loss) y la precisi√≥n (accuracy) del modelo en entrenamiento y validaci√≥n a lo largo de las √©pocas. Esto permite evaluar el aprendizaje de la red, detectar overfitting o underfitting, y comparar el desempe√±o entre diferentes configuraciones del modelo.\n",
    "\n",
    "La precisi√≥n se calcula como:\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{N√∫mero de predicciones correctas}}{\\text{N√∫mero total de predicciones}} \\times 100\\%\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dff22fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history, title):\n",
    "    \"\"\"\n",
    "    Dibuja las curvas de loss y accuracy (train y dev) guardadas en el diccionario 'history'.\n",
    "    Espera claves: 'train_loss', 'train_acc', 'val_loss', 'val_acc'.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    # --- Loss ---\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history['train_loss'], label='Train Loss')\n",
    "    plt.plot(epochs, history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training and Validation Loss - {title}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # --- Accuracy ---\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history['train_acc'], label='Train Accuracy')\n",
    "    plt.plot(epochs, history['val_acc'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title(f'Training and Validation Accuracy - {title}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd0dd3",
   "metadata": {},
   "source": [
    "#### Matriz de confusi√≥n\n",
    "Asimismo, la siguiente funci√≥n muestra la comparaci√≥n entre las etiquetas reales (`y_true`) y las predicciones del modelo (`y_pred`) para cada clase. Permite identificar qu√© clases se confunden entre s√≠ y evaluar detalladamente el desempe√±o del modelo en clasificaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e2acc450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, title=\"Confusion Matrix\"):\n",
    "    if y_true is None or y_pred is None: return\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(title); plt.ylabel('Etiqueta Real'); plt.xlabel('Etiqueta Predicha')\n",
    "    plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60bb93f",
   "metadata": {},
   "source": [
    "#### Guardar campos en el CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "68f5403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def save_results(all_results, dataset_name=\"dataset\", use_DA=False, model_type=\"scratch\"):\n",
    "    \"\"\"\n",
    "    Guarda resultados en un CSV cuyo nombre depende del dataset,\n",
    "    si se us√≥ Data Augmentation y el tipo de modelo:\n",
    "        - scratch  (cnn desde cero)\n",
    "        - transfer (transfer learning)\n",
    "        - otro\n",
    "    \"\"\"\n",
    "\n",
    "    if not all_results:\n",
    "        return\n",
    "\n",
    "    # sufijos din√°micos\n",
    "    da_suffix = \"_DA\" if use_DA else \"_NoDA\"\n",
    "    \n",
    "    if model_type == \"scratch\":\n",
    "        model_suffix = \"\"\n",
    "    else:\n",
    "        model_suffix = f\"_{model_type}\"\n",
    "\n",
    "    csv_filename = f\"resultados_{dataset_name}{da_suffix}{model_suffix}.csv\"\n",
    "\n",
    "    # Convertir resultados a DataFrame columna √∫nica\n",
    "    df_results = pd.DataFrame(all_results)\n",
    "    df_results[\"history_dict\"] = df_results[\"history_dict\"].apply(json.dumps)\n",
    "    df_results[\"y_true\"] = df_results[\"y_true\"].apply(lambda x: json.dumps(x.tolist()))\n",
    "    df_results[\"y_pred\"] = df_results[\"y_pred\"].apply(lambda x: json.dumps(x.tolist()))\n",
    "\n",
    "    df_csv = df_results.T\n",
    "\n",
    "    # Funci√≥n para reemplazar puntos por comas\n",
    "    def decimal_to_comma(df):\n",
    "        return df.applymap(lambda x: str(x).replace('.', ',') if isinstance(x, (float, int)) else x)\n",
    "\n",
    "    # Si el archivo no existe ‚Üí se crea\n",
    "    if not os.path.exists(csv_filename):\n",
    "        df_to_save = decimal_to_comma(df_csv)\n",
    "        df_to_save.to_csv(csv_filename, sep=';', index=True)\n",
    "\n",
    "    # Si ya existe ‚Üí se agrega una columna nueva\n",
    "    else:\n",
    "        df_prev = pd.read_csv(csv_filename, sep=';', index_col=0)\n",
    "        df_new = decimal_to_comma(df_csv)\n",
    "\n",
    "        # Alinear e insertar\n",
    "        df_prev, df_new = df_prev.align(df_new, join='outer')\n",
    "        df_combined = pd.concat([df_prev, df_new], axis=1)\n",
    "\n",
    "        df_combined.to_csv(csv_filename, sep=';', index=True)\n",
    "\n",
    "    print(f\"‚úî Resultados guardados en: {csv_filename}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7af677b",
   "metadata": {},
   "source": [
    "#### Leer desde el CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "48dd046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def robust_json_load(x):\n",
    "    \"\"\"\n",
    "    Intenta cargar JSON de forma estricta. Si falla, intenta corregir\n",
    "    formatos comunes (comillas simples, booleanos de Python, etc.)\n",
    "    usando ast.literal_eval.\n",
    "    \"\"\"\n",
    "    if not isinstance(x, str):\n",
    "        return x\n",
    "    try:\n",
    "        return json.loads(x)\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        try:\n",
    "            return ast.literal_eval(x.replace(\"null\", \"None\").replace(\"true\", \"True\").replace(\"false\", \"False\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Error decodificando: {e}\")\n",
    "            return None\n",
    "\n",
    "def load_all_experiments_from_csv(csv_filename):\n",
    "    # 1. Leer con el separador correcto (punto y coma seg√∫n tu archivo)\n",
    "    df = pd.read_csv(csv_filename, sep=';', index_col=0)\n",
    "    \n",
    "    # 2. Transponer (Filas = Experimentos)\n",
    "    df = df.T\n",
    "    \n",
    "    # 3. Convertir columnas JSON de forma robusta\n",
    "    json_cols = [\"history_dict\", \"y_true\", \"y_pred\"]\n",
    "    for col in json_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(robust_json_load)\n",
    "            \n",
    "    # 4. Convertir test_acc a float manejando la coma decimal\n",
    "    if \"test_acc\" in df.columns:\n",
    "        # Reemplaza ',' por '.' y convierte a float\n",
    "        df[\"test_acc\"] = df[\"test_acc\"].astype(str).str.replace(',', '.').astype(float)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c2b077",
   "metadata": {},
   "source": [
    "#### Analizar mejor y peor experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "67c880e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_from_csv(csv_filename, classes):\n",
    "    df = pd.read_csv(csv_filename, sep=';', index_col=0).T\n",
    "    json_cols = [\"history_dict\", \"y_true\", \"y_pred\"]\n",
    "    for col in json_cols:\n",
    "        if col in df.columns: df[col] = df[col].apply(robust_json_load)\n",
    "    if \"test_acc\" in df.columns:\n",
    "        df[\"test_acc\"] = df[\"test_acc\"].astype(str).str.replace(',', '.').astype(float)\n",
    "\n",
    "    best = df.loc[df[\"test_acc\"].idxmax()]\n",
    "    worst = df.loc[df[\"test_acc\"].idxmin()]\n",
    "\n",
    "    print(f\"üèÜ Mejor: {best['name']} - {best['test_acc']}%\")\n",
    "    print(f\"üìâ Peor: {worst['name']} - {worst['test_acc']}%\")\n",
    "\n",
    "    plot_training_history(best[\"history_dict\"], f\"Mejor ({best['name']})\")\n",
    "    plot_confusion_matrix(best[\"y_true\"], best[\"y_pred\"], classes, f\"Confusion ({best['name']})\")\n",
    "\n",
    "    plot_training_history(worst[\"history_dict\"], f\"Peor ({worst['name']})\")\n",
    "    plot_confusion_matrix(worst[\"y_true\"], worst[\"y_pred\"], classes, f\"Confusion ({worst['name']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d367eb26",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Entrenamiento CNN desde cero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e77965",
   "metadata": {},
   "source": [
    "#### Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232a08d2",
   "metadata": {},
   "source": [
    "Este bloque define un **modelo CNN completamente modular y configurable**. Permite ajustar el n√∫mero de capas convolucionales, filtros, kernel sizes, repeticiones, y tambi√©n la estructura de las capas densas.\n",
    "\n",
    "- **Bloques convolucionales**: cada bloque aplica varias Conv2D + LeakyReLU para extraer caracter√≠sticas, seguido de MaxPool para reducir la resoluci√≥n espacial y Dropout para evitar sobreajuste.\n",
    "\n",
    "- **C√°lculo autom√°tico del tama√±o plano**: se usa un `dummy_input` para pasar por la parte convolucional y obtener autom√°ticamente el tama√±o exacto que debe recibir la primera capa densa.\n",
    "\n",
    "- **Capas densas (FC)**: construyen un clasificador configurable, con activaciones LeakyReLU y Dropout para regularizaci√≥n.\n",
    "\n",
    "- **Capa final**: genera la salida final del modelo, lista para usar con `CrossEntropyLoss` (por eso no contiene Softmax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ba943f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ModularCNN(nn.Module):\n",
    "    def __init__(self, input_shape, conv_layers, fc_layers, num_classes, dropout_conv=0.3, dropout_fc=0.4):\n",
    "        super(ModularCNN, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential()\n",
    "        in_channels = input_shape[2]\n",
    "        \n",
    "        # --- BLOQUES CONVOLUCIONALES ---\n",
    "        for i, (out_channels, k, reps) in enumerate(conv_layers):\n",
    "            for _ in range(reps):\n",
    "                self.features.add_module(f\"conv_{i}_{_}\", nn.Conv2d(in_channels, out_channels, kernel_size=k, padding=k//2))\n",
    "                self.features.add_module(f\"relu_{i}_{_}\", nn.LeakyReLU(0.1))\n",
    "                in_channels = out_channels\n",
    "            \n",
    "            self.features.add_module(f\"pool_{i}\", nn.MaxPool2d(2, 2))\n",
    "            self.features.add_module(f\"drop_{i}\", nn.Dropout(dropout_conv))\n",
    "\n",
    "        # --- CAPAS DENSAS ---\n",
    "        self._to_linear = None\n",
    "        dummy_input = torch.zeros(1, input_shape[2], input_shape[0], input_shape[1])\n",
    "        dummy_out = self.features(dummy_input)\n",
    "        self.flat_dim = int(np.prod(dummy_out.size()))\n",
    "        \n",
    "        self.classifier = nn.Sequential()\n",
    "        in_features = self.flat_dim\n",
    "        \n",
    "        for i, units in enumerate(fc_layers):\n",
    "            self.classifier.add_module(f\"fc_{i}\", nn.Linear(in_features, units))\n",
    "            self.classifier.add_module(f\"relu_fc_{i}\", nn.LeakyReLU(0.1))\n",
    "            self.classifier.add_module(f\"drop_fc_{i}\", nn.Dropout(dropout_fc))\n",
    "            in_features = units\n",
    "            \n",
    "        # Capa final (Sin Softmax, CrossEntropyLoss lo aplica internamente)\n",
    "        self.classifier.add_module(\"fc_out\", nn.Linear(in_features, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca95a9cd",
   "metadata": {},
   "source": [
    "#### Entrenar modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4798056",
   "metadata": {},
   "source": [
    "##### Entreno de una √©poca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf224ce",
   "metadata": {},
   "source": [
    "`train_one_epoch`: Entrena el modelo durante una √©poca completa.\n",
    "\n",
    "**Pasos principales:**\n",
    "- `model.train()`: activa el modo entrenamiento (Dropout, BatchNorm).\n",
    "- Itera sobre el `DataLoader` para obtener lotes de im√°genes y etiquetas.\n",
    "- Mueve los datos al dispositivo (`CPU/GPU`).\n",
    "- Realiza el **forward** (`outputs = model(images)`).\n",
    "- Calcula la **p√©rdida** con la funci√≥n `criterion`.\n",
    "- Hace **backpropagation** (`loss.backward()`).\n",
    "- Actualiza los pesos con `optimizer.step()`.\n",
    "- Acumula p√©rdida y aciertos para calcular:\n",
    "  - **epoch_loss**: p√©rdida media de la √©poca.\n",
    "  - **epoch_acc**: accuracy de la √©poca.\n",
    "\n",
    "**Salida**:  `(epoch_loss, epoch_acc)`\n",
    "\n",
    "`validate`: Eval√∫a el modelo en el conjunto de validaci√≥n sin actualizar pesos. \n",
    "\n",
    "**Pasos principales:**\n",
    "- `model.eval()`: modo evaluaci√≥n (Dropout desactivado).\n",
    "- `with torch.no_grad()`: no calcula gradientes para ahorrar memoria/tiempo.\n",
    "- Itera sobre el `DataLoader`.\n",
    "- Calcula:\n",
    "  - **Loss total** del conjunto.\n",
    "  - **Accuracy** basado en predicciones correctas.\n",
    "\n",
    "**Salida**:  `(val_loss, val_acc)`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "cde6b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    return running_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5288b7",
   "metadata": {},
   "source": [
    "##### EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c985dcd5",
   "metadata": {},
   "source": [
    "La clase `EarlyStopping` sirve para **detener el entrenamiento de un modelo autom√°ticamente** si la m√©trica de inter√©s (por ejemplo, accuracy de validaci√≥n) deja de mejorar durante varias √©pocas consecutivas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "00fe5da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=8, mode='max', verbose=False):\n",
    "        self.patience = patience\n",
    "        self.mode = mode\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, current_score, model):\n",
    "        score = current_score if self.mode == 'max' else -current_score\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict()\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "\n",
    "    def load_best_weights(self, model):\n",
    "        if self.best_model_state:\n",
    "            model.load_state_dict(self.best_model_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063e66c3",
   "metadata": {},
   "source": [
    "##### Entrenamiento completo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6136df60",
   "metadata": {},
   "source": [
    "Esta funci√≥n realiza un **entrenamiento completo del modelo** con validaci√≥n y early stopping.\n",
    "\n",
    "**Pasos principales:**\n",
    "\n",
    "1. **Definir criterio y optimizador**  \n",
    "   - `criterion = CrossEntropyLoss()` ‚Üí calcula la p√©rdida de clasificaci√≥n.  \n",
    "   - `optimizer = Adam` ‚Üí actualiza los pesos del modelo.\n",
    "\n",
    "2. **EarlyStopping**  \n",
    "   - Monitorea la m√©trica de validaci√≥n (`val_acc`).  \n",
    "   - Detiene el entrenamiento si no mejora durante `patience` √©pocas.  \n",
    "   - Guarda los mejores pesos autom√°ticamente.\n",
    "\n",
    "3. **Entrenamiento por √©pocas**  \n",
    "   Para cada √©poca:\n",
    "   - `train_one_epoch`: realiza entrenamiento completo de la √©poca.\n",
    "   - `validate`: eval√∫a el modelo en el conjunto de validaci√≥n.\n",
    "   - Se registran las m√©tricas (`loss` y `accuracy`) en el historial.\n",
    "   - Se imprime el progreso de la √©poca.\n",
    "\n",
    "4. **Chequeo de EarlyStopping**  \n",
    "   - Si la m√©trica de validaci√≥n no mejora, se detiene el entrenamiento antes de llegar al n√∫mero m√°ximo de √©pocas.\n",
    "\n",
    "5. **Cargar los mejores pesos**  \n",
    "   - Despu√©s de terminar, `es.load_best_weights(model)` restaura los pesos del modelo con mejor rendimiento en validaci√≥n.\n",
    "\n",
    "**Salida:**  \n",
    "- `history`: diccionario con listas de p√©rdidas y accuracies para entrenamiento y validaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "89cdab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train_experiment(model, train_loader, val_loader, epochs, lr, patience, device, class_weights_tensor):\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    es = EarlyStopping(patience=patience, mode='max', verbose=True)\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        t_loss, t_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        v_loss, v_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        history['train_loss'].append(t_loss)\n",
    "        history['train_acc'].append(t_acc)\n",
    "        history['val_loss'].append(v_loss)\n",
    "        history['val_acc'].append(v_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {t_loss:.4f} - Acc: {t_acc:.4f} - Val Loss: {v_loss:.4f} - Val Acc: {v_acc:.4f}\")\n",
    "        \n",
    "        es(v_acc, model)\n",
    "        if es.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "            \n",
    "    es.load_best_weights(model)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a89adac",
   "metadata": {},
   "source": [
    "#### Evaluar modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ba5b98",
   "metadata": {},
   "source": [
    "Esta funci√≥n **eval√∫a el rendimiento del modelo** en un conjunto de datos (por ejemplo, test o validaci√≥n) y devuelve m√©tricas detalladas.\n",
    "\n",
    "**Pasos principales:**\n",
    "- `model.eval()`: modo evaluaci√≥n (Dropout desactivado).\n",
    "- `with torch.no_grad()`: no calcula gradientes para ahorrar memoria/tiempo.\n",
    "- Funci√≥n de p√©rdida: `CrossEntropyLoss()\n",
    "- Itera sobre el `DataLoader`.\n",
    "    Para cada lote:\n",
    "    - Mueve im√°genes y etiquetas al dispositivo (CPU/GPU)\n",
    "    - Calcula las predicciones (outputs = model(images)).\n",
    "    - Calcula la p√©rdida (loss = criterion(outputs, labels)).\n",
    "    - Guarda predicciones y etiquetas reales.\n",
    "- Calcula m√©tricas finales:\n",
    "  - **Loss total**, p√©rdida promedio del conjunto.\n",
    "  - **Accuracy** basado en predicciones correctas\n",
    "\n",
    "**Salida**: \n",
    "- **acc**: accuracy total en porcentaje.\n",
    "- **avg_loss**: p√©rdida promedio.\n",
    "- **all_labels**: array con todas las etiquetas verdaderas.\n",
    "- **all_preds**: array con todas las predicciones del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2163523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, device, class_weights_tensor):\n",
    "    model.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    total = len(all_labels)\n",
    "    acc = np.sum(np.array(all_preds) == np.array(all_labels)) / total * 100\n",
    "    avg_loss = running_loss / total\n",
    "    \n",
    "    return acc, avg_loss, np.array(all_labels), np.array(all_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854049f2",
   "metadata": {},
   "source": [
    "#### Entrenamientos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8214979a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando GPU: NVIDIA GeForce RTX 3060\n",
      "Descargando dataset (FER-2013)...\n",
      "N√∫mero de im√°genes de entrenamiento: 28709\n",
      "N√∫mero de im√°genes de validaci√≥n: 3589\n",
      "N√∫mero de im√°genes de test: 3589\n",
      "\n",
      "====================================\n",
      "Entrenando experimento: Experimento 1\n",
      "====================================\n",
      "\n",
      "Epoch 1/200 - Loss: 1.4020 - Acc: 0.1418 - Val Loss: 2.7482 - Val Acc: 0.0156\n",
      "Epoch 2/200 - Loss: 1.3533 - Acc: 0.1427 - Val Loss: 2.7339 - Val Acc: 0.0156\n",
      "Epoch 3/200 - Loss: 1.1873 - Acc: 0.1914 - Val Loss: 2.1126 - Val Acc: 0.1148\n",
      "Epoch 4/200 - Loss: 0.9352 - Acc: 0.2910 - Val Loss: 1.9465 - Val Acc: 0.2151\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "from torch import classes\n",
    "\n",
    "def main():\n",
    "    \"\"\"Funci√≥n principal optimizada para maximizar Accuracy en FER-2013,\n",
    "    con guardado completo en CSV y an√°lisis desde hist√≥rico.\"\"\"\n",
    "    \n",
    "    # --- Configuraci√≥n global ---\n",
    "    BATCH_SIZE = 64\n",
    "    INPUT_CHANNELS = 1\n",
    "    EPOCHS = 200\n",
    "    SEED=42\n",
    "    DEVICE = device()\n",
    "    IMG_SIZE = (48, 48)\n",
    "\n",
    "    if DEVICE.type == 'cuda':\n",
    "        print(f\"Usando GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"Usando CPU\")\n",
    "        \n",
    "    # DEFINICI√ìN DE EXPERIMENTOS\n",
    "    EXPERIMENTS = [\n",
    "        {\n",
    "            \"name\": \"Experimento 1\",\n",
    "            \"conv_layers\": [\n",
    "                (64, 3, 2),   \n",
    "                (128, 3, 2), \n",
    "                (256, 3, 2),  \n",
    "                (512, 3, 2)   \n",
    "            ],\n",
    "            \"fc_layers\": [512, 256], \n",
    "            \"lr\": 1e-4,      \n",
    "            \"dropout_conv\": 0.3,\n",
    "            \"dropout_fc\": 0.5,    \n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Experimento 2\",\n",
    "            \"conv_layers\": [\n",
    "                (32, 3, 1), \n",
    "                (64, 3, 1), \n",
    "                (128, 3, 1), \n",
    "                (256, 3, 1),\n",
    "                (512, 3, 1)\n",
    "            ],\n",
    "            \"fc_layers\": [1024, 256], \n",
    "            \"lr\": 2e-4,\n",
    "            \"dropout_conv\": 0.25,\n",
    "            \"dropout_fc\": 0.5,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Experimento 3\",\n",
    "            \"conv_layers\": [\n",
    "                (64, 3, 1),\n",
    "                (128, 3, 2),\n",
    "                (256, 3, 2),\n",
    "                (512, 3, 2)\n",
    "            ],\n",
    "            \"fc_layers\": [512, 256],\n",
    "            \"lr\": 1e-4,\n",
    "            \"dropout_conv\": 0.25,\n",
    "            \"dropout_fc\": 0.5,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Experimento 4\",\n",
    "            \"conv_layers\": [\n",
    "                (32, 3, 1),\n",
    "                (64, 3, 1),\n",
    "                (128, 3, 1),\n",
    "                (256, 3, 1)\n",
    "            ],\n",
    "            \"fc_layers\": [256],\n",
    "            \"lr\": 1e-3,\n",
    "            \"dropout_conv\": 0.25,\n",
    "            \"dropout_fc\": 0.4,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Experimento 5\",\n",
    "            \"conv_layers\": [\n",
    "                (64, 3, 1),\n",
    "                (128, 3, 2),\n",
    "                (256, 3, 2),\n",
    "                (512, 3, 2)\n",
    "            ],\n",
    "            \"fc_layers\": [512, 256],\n",
    "            \"lr\": 1e-4,\n",
    "            \"dropout_conv\": 0.2,\n",
    "            \"dropout_fc\": 0.45,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Experimento 6\",\n",
    "            \"conv_layers\": [\n",
    "                (64, 3, 2),\n",
    "                (128, 3, 2),\n",
    "                (256, 3, 4), \n",
    "                (512, 3, 4),  \n",
    "            ],\n",
    "            \"fc_layers\": [1024, 512],\n",
    "            \"lr\": 5e-5,\n",
    "            \"dropout_conv\": 0.25,\n",
    "            \"dropout_fc\": 0.5,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # 1. SETUP & DATOS\n",
    "    seed_everything(SEED)\n",
    "\n",
    "    # Dataset FER-2013 desde Kaggle\n",
    "    print(\"Descargando dataset (FER-2013)...\")\n",
    "    dataset_path = kagglehub.dataset_download(\"pankaj4321/fer-2013-facial-expression-dataset\")\n",
    "\n",
    "    # Dataset FER-2013 modificado\n",
    "    # print(\"Descargando dataset modificado (FER-2013)...\")\n",
    "    # dataset_path = r\"C:\\Users\\Laura\\Downloads\\datos\"\n",
    "\n",
    "    # Preparar datos sin data augmentation\n",
    "    USE_DA = False\n",
    "    train_loader, val_loader, test_loader, classes, class_weights_tensor = prepare_dataloaders(\n",
    "        dataset_path=dataset_path,\n",
    "        input_channels=INPUT_CHANNELS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        img_size=(48, 48),\n",
    "    )\n",
    "\n",
    "    # Preparar datos con data augmentation\n",
    "    # USE_DA = True\n",
    "    #train_loader, val_loader, test_loader, classes, class_weights_tensor = prepare_dataloaders_data_augmentation(\n",
    "    #   dataset_path=dataset_path,\n",
    "    #   input_channels=INPUT_CHANNELS,\n",
    "    #   batch_size=BATCH_SIZE,\n",
    "    #   img_size=IMG_SIZE,\n",
    "    #)\n",
    "\n",
    "    print(f\"N√∫mero de im√°genes de entrenamiento: {len(train_loader.dataset)}\")\n",
    "    print(f\"N√∫mero de im√°genes de validaci√≥n: {len(val_loader.dataset)}\")\n",
    "    print(f\"N√∫mero de im√°genes de test: {len(test_loader.dataset)}\")\n",
    "\n",
    "    all_results = []\n",
    "    input_shape = (48, 48, INPUT_CHANNELS)\n",
    "    \n",
    "    # 2. EJECUCI√ìN DE EXPERIMENTOS\n",
    "    for exp in EXPERIMENTS:\n",
    "        print(\"\\n====================================\")\n",
    "        print(f\"Entrenando experimento: {exp['name']}\")\n",
    "        print(\"====================================\\n\")\n",
    "\n",
    "        # A. CONSTRUIR\n",
    "        model = ModularCNN(\n",
    "            input_shape=input_shape,\n",
    "            conv_layers=exp[\"conv_layers\"],\n",
    "            fc_layers=exp[\"fc_layers\"],\n",
    "            dropout_conv=exp.get(\"dropout_conv\", 0.3),\n",
    "            dropout_fc=exp.get(\"dropout_fc\", 0.4),\n",
    "            num_classes=len(classes)\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # B. ENTRENAR\n",
    "        history = train_experiment(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            epochs=EPOCHS,\n",
    "            lr=exp[\"lr\"],\n",
    "            patience=10,\n",
    "            device=DEVICE,\n",
    "            class_weights_tensor=class_weights_tensor\n",
    "        )\n",
    "\n",
    "        # D. EVALUAR\n",
    "        test_acc, test_loss, y_true, y_pred = evaluate_model(model, test_loader, device=DEVICE, class_weights_tensor=class_weights_tensor)\n",
    "        print(f\"[{exp['name']}] Test Accuracy = {test_acc:.4f}%\")\n",
    "\n",
    "        # E. ALMACENAR RESULTADOS\n",
    "        result_entry = {\n",
    "            \"name\": exp['name'],\n",
    "            \"conv_layers_config\": str(exp[\"conv_layers\"]),\n",
    "            \"fc_layers_config\": str(exp[\"fc_layers\"]),\n",
    "            \"learning_rate\": exp[\"lr\"],\n",
    "            \"dropout_conv\": exp.get(\"dropout_conv\", 0.3),\n",
    "            \"dropout_fc\": exp.get(\"dropout_fc\", 0.4),\n",
    "            \"epochs_run\": len(history[\"train_loss\"]),\n",
    "            \"test_loss\": test_loss,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"history_dict\": history,\n",
    "            \"y_true\": y_true,\n",
    "            \"y_pred\": y_pred\n",
    "        }\n",
    "        all_results.append(result_entry)\n",
    "\n",
    "    # 3. GUARDAR EN CSV\n",
    "    if \"fer-2013\" in dataset_path:\n",
    "        save_results(all_results, dataset_name=\"cnn_fer_original\", use_DA=USE_DA, model_type=\"scratch\")\n",
    "    else:\n",
    "        save_results(all_results, dataset_name=\"cnn_fer_modificado\", use_DA=USE_DA, model_type=\"scratch\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befbcb5e",
   "metadata": {},
   "source": [
    "#### An√°lisis distintos experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47796f48",
   "metadata": {},
   "source": [
    "##### Dataset original sin DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56292856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "dataset_path = kagglehub.dataset_download(\"pankaj4321/fer-2013-facial-expression-dataset\")\n",
    "BATCH_SIZE = 64\n",
    "INPUT_CHANNELS = 1\n",
    "\n",
    "train_loader, val_loader, test_loader, classes = prepare_dataloaders(\n",
    "    dataset_path=dataset_path,\n",
    "    input_channels=INPUT_CHANNELS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    img_size=(48, 48)\n",
    ")\n",
    "\n",
    "try:\n",
    "    print(\"Iniciando an√°lisis (FER-2013 original sin DA)...\")\n",
    "    analyze_from_csv(\"resultados_cnn_fer_original_NoDA.csv\", classes)\n",
    "    print(\"‚úÖ An√°lisis completado.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error durante el an√°lisis: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f64686",
   "metadata": {},
   "source": [
    "##### Dataset modificado sin DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bdd3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r\"C:\\Users\\Laura\\Downloads\\datos\"\n",
    "BATCH_SIZE = 64\n",
    "INPUT_CHANNELS = 1\n",
    "\n",
    "train_loader, val_loader, test_loader, classes = prepare_dataloaders(\n",
    "    dataset_path=dataset_path,\n",
    "    input_channels=INPUT_CHANNELS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    img_size=(48, 48)\n",
    ")\n",
    "\n",
    "try:\n",
    "    print(\"Iniciando an√°lisis (FER-2013 modificado sin DA)...\")\n",
    "    analyze_from_csv(\"resultados_cnn_fer_modificado_NoDA.csv\", classes)\n",
    "    print(\"‚úÖ An√°lisis completado.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error durante el an√°lisis: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2633caf",
   "metadata": {},
   "source": [
    "##### Dataset original con DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb32e364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "dataset_path = kagglehub.dataset_download(\"pankaj4321/fer-2013-facial-expression-dataset\")\n",
    "BATCH_SIZE = 64\n",
    "INPUT_CHANNELS = 1\n",
    "\n",
    "train_loader, val_loader, test_loader, classes = prepare_dataloaders_data_augmentation(\n",
    "    dataset_path=dataset_path,\n",
    "    input_channels=INPUT_CHANNELS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    img_size=(48, 48)\n",
    ")\n",
    "\n",
    "try:\n",
    "    print(\"Iniciando an√°lisis (FER-2013 original con DA)...\")\n",
    "    analyze_from_csv(\"resultados_cnn_fer_original_DA.csv\", classes)\n",
    "    print(\"‚úÖ An√°lisis completado.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error durante el an√°lisis: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aad9d9",
   "metadata": {},
   "source": [
    "##### Dataset modificado con DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed45dbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r\"C:\\Users\\Laura\\Downloads\\datos\"\n",
    "BATCH_SIZE = 64\n",
    "INPUT_CHANNELS = 1\n",
    "\n",
    "train_loader, val_loader, test_loader, classes = prepare_dataloaders_data_augmentation(\n",
    "    dataset_path=dataset_path,\n",
    "    input_channels=INPUT_CHANNELS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    img_size=(48, 48)\n",
    ")\n",
    "\n",
    "try:\n",
    "    print(\"Iniciando an√°lisis (FER-2013 modificado con DA)...\")\n",
    "    analyze_from_csv(\"resultados_cnn_fer_modificado_DA.csv\", classes)\n",
    "    print(\"‚úÖ An√°lisis completado.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error durante el an√°lisis: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f84c1da",
   "metadata": {},
   "source": [
    "## Entrenamiento CNN con Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf33d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "def build_model(num_classes, device):\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf23b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "def main():\n",
    "\n",
    "    # --- Configuraci√≥n global ---\n",
    "    BATCH_SIZE = 64\n",
    "    INPUT_CHANNELS = 3\n",
    "    EPOCHS = 200\n",
    "    SEED = 42\n",
    "    DEVICE = device()\n",
    "    IMG_SIZE = (48, 48)\n",
    "\n",
    "    EXPERIMENTOS = [\n",
    "        {\n",
    "            \"name\": \"Experimento 1\",\n",
    "            \"lr\": 1e-4,      \n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Experimento 2\", \n",
    "            \"lr\": 2e-4,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Experimento 3\",\n",
    "            \"lr\": 1e-3,      \n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Experimento 4\",\n",
    "            \"lr\": 5e-5,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # 1. SETUP & DATOS\n",
    "    seed_everything(SEED)\n",
    "\n",
    "    # Dataset FER-2013 desde Kaggle\n",
    "    dataset_path = kagglehub.dataset_download(\n",
    "        \"pankaj4321/fer-2013-facial-expression-dataset\"\n",
    "    )\n",
    "\n",
    "    # Preparar datos con data augmentation\n",
    "    train_loader, val_loader, test_loader, classes = prepare_dataloaders_data_augmentation(\n",
    "        dataset_path=dataset_path,\n",
    "        input_channels=INPUT_CHANNELS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        img_size=IMG_SIZE\n",
    "    )\n",
    "\n",
    "    print(f\"N√∫mero de im√°genes de entrenamiento: {len(train_loader.dataset)}\")\n",
    "    print(f\"N√∫mero de im√°genes de validaci√≥n: {len(val_loader.dataset)}\")\n",
    "    print(f\"N√∫mero de im√°genes de test: {len(test_loader.dataset)}\")\n",
    "    \n",
    "    all_results = []\n",
    "\n",
    "    # 2. EJECUCI√ìN DE EXPERIMENTOS\n",
    "    for exp in EXPERIMENTOS:\n",
    "        print(\"\\n====================================\")\n",
    "        print(f\"Entrenando experimento: {exp['name']}\")\n",
    "        print(\"====================================\\n\")\n",
    "\n",
    "        # A. CONSTRUIR MODELO\n",
    "        model = build_model(len(classes), DEVICE)\n",
    "\n",
    "        # B. ENTRENAR\n",
    "        history = train_experiment(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            epochs=EPOCHS,\n",
    "            lr=exp[\"lr\"],\n",
    "            patience=10,           \n",
    "            device=DEVICE\n",
    "        )\n",
    "\n",
    "        # D. EVALUAR\n",
    "        test_acc, test_loss, y_true, y_pred = evaluate_model(model, test_loader, device=DEVICE)\n",
    "        print(f\"[{exp['name']}] Test Accuracy = {test_acc:.4f}%\")\n",
    "\n",
    "        # E. ALMACENAR RESULTADOS\n",
    "        result_entry = {\n",
    "            \"name\": exp['name'],\n",
    "            \"epochs_run\": len(history[\"train_loss\"]),\n",
    "            \"test_loss\": test_loss,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"history_dict\": history,\n",
    "            \"y_true\": y_true, \n",
    "            \"y_pred\": y_pred\n",
    "        }\n",
    "        \n",
    "        all_results.append(result_entry)\n",
    "\n",
    "    # 3. GUARDAR EN CSV\n",
    "    save_results(all_results, dataset_name=\"cnn_fer_original\", use_DA=True, model_type=\"transfer\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a4d93a",
   "metadata": {},
   "source": [
    "#### An√°lisis experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad311b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "dataset_path = kagglehub.dataset_download(\"pankaj4321/fer-2013-facial-expression-dataset\")\n",
    "BATCH_SIZE = 64\n",
    "INPUT_CHANNELS = 1\n",
    "\n",
    "train_loader, val_loader, test_loader, classes = prepare_dataloaders_data_augmentation(\n",
    "    dataset_path=dataset_path,\n",
    "    input_channels=INPUT_CHANNELS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    img_size=(48, 48)\n",
    ")\n",
    "\n",
    "try:\n",
    "    print(\"Iniciando an√°lisis (FER-2013 original con DA)...\")\n",
    "    analyze_from_csv(\"resultados_cnn_fer_original_DA_transfer.csv\", classes)\n",
    "    print(\"‚úÖ An√°lisis completado.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error durante el an√°lisis: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FSI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
