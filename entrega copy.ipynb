{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dcac814",
   "metadata": {},
   "source": [
    "# Pr√°ctica 1 - Red Neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5ab944",
   "metadata": {},
   "source": [
    "### Utilidades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecfe126",
   "metadata": {},
   "source": [
    "#### Configuraci√≥n del dispositivo de c√≥mputo\n",
    "Selecciona autom√°ticamente GPU si est√° disponible, o CPU en caso contrario, e imprime el dispositivo usado para entrenar la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0fb26117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Usando dispositivo:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e869ebd",
   "metadata": {},
   "source": [
    "#### Fijaci√≥n de semilla para reproducibilidad\n",
    "La siguiente funci√≥n, `seed_everything`, asegura que los experimentos sean reproducibles fijando la semilla para PyTorch, CUDA, NumPy y el generador de n√∫meros aleatorios de Python. Tambi√©n configura CUDA para un comportamiento determinista cuando se usa GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e2cecf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370630db",
   "metadata": {},
   "source": [
    "#### Preparaci√≥n de datasets y dataloaders\n",
    "La funci√≥n `prepare_dataloaders` utiliza TensorFlow/Keras para cargar im√°genes desde un directorio (dataset_path), dividi√©ndolas en conjuntos de entrenamiento, validaci√≥n y prueba bas√°ndose en la estructura de subdirectorios (train, val y test). \n",
    "\n",
    "Aplica un preprocesamiento b√°sico a las im√°genes, como redimensionarlas a $48 \\times 48$ p√≠xeles, convertirlas a escala de grises (o el modo de color especificado), y las organiza en lotes (batch_size=64). \n",
    "\n",
    "Adem√°s, utiliza la codificaci√≥n one-hot (label_mode='categorical') para las etiquetas, mezcla el conjunto de entrenamiento (shuffle=True) para asegurar la aleatoriedad, y aplica optimizaciones de carga de datos (cache().prefetch()) para mejorar la velocidad del entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf3227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "def prepare_dataloaders(dataset_path: str,\n",
    "                           batch_size=64,\n",
    "                           img_size=(48, 48),\n",
    "                           color_mode='grayscale'): \n",
    "    \n",
    "    train_dir = os.path.join(dataset_path, 'train')\n",
    "    val_dir = os.path.join(dataset_path, 'val') \n",
    "    test_dir = os.path.join(dataset_path, 'test') \n",
    "\n",
    "    # Carga entrenamiento\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_dir,\n",
    "        labels='inferred',\n",
    "        label_mode='categorical', # Importante para categorical_crossentropy\n",
    "        color_mode=color_mode,\n",
    "        batch_size=batch_size,\n",
    "        image_size=img_size,\n",
    "        shuffle=True,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Carga validaci√≥n\n",
    "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        val_dir,\n",
    "        labels='inferred',\n",
    "        label_mode='categorical',\n",
    "        color_mode=color_mode,\n",
    "        batch_size=batch_size,\n",
    "        image_size=img_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Carga test\n",
    "    test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        test_dir,\n",
    "        labels='inferred',\n",
    "        label_mode='categorical',\n",
    "        color_mode=color_mode,\n",
    "        batch_size=batch_size,\n",
    "        image_size=img_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    classes = train_ds.class_names\n",
    "    \n",
    "    # Optimizaci√≥n de carga (cache y prefetch)\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return train_ds, val_ds, test_ds, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee832bae",
   "metadata": {},
   "source": [
    "#### Gr√°fico de las curvas de accuracy durante el entrenamiento y la validaci√≥n de la red neuronal.\n",
    "A continuaci√≥n, se definir√° una funci√≥n que muestre c√≥mo cambian la p√©rdida (loss) y la precisi√≥n (accuracy) del modelo en entrenamiento y validaci√≥n a lo largo de las √©pocas. Esto permite evaluar el aprendizaje de la red, detectar overfitting o underfitting, y comparar el desempe√±o entre diferentes configuraciones del modelo.\n",
    "\n",
    "La precisi√≥n se calcula como:\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{N√∫mero de predicciones correctas}}{\\text{N√∫mero total de predicciones}} \\times 100\\%\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dff22fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history, title):\n",
    "    if history is None:\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Gr√°fico de P√©rdida (Loss)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.get('train_loss', []), label='Train Loss')\n",
    "    plt.plot(history.get('val_loss', []), label='Val Loss')\n",
    "    plt.title(f'Loss - {title}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Gr√°fico de Precisi√≥n (Accuracy)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.get('train_acc', []), label='Train Acc')\n",
    "    plt.plot(history.get('val_acc', []), label='Val Acc')\n",
    "    plt.title(f'Accuracy - {title}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd0dd3",
   "metadata": {},
   "source": [
    "#### Matriz de confusi√≥n\n",
    "Asimismo, la siguiente funci√≥n muestra la comparaci√≥n entre las etiquetas reales (`y_true`) y las predicciones del modelo (`y_pred`) para cada clase. Permite identificar qu√© clases se confunden entre s√≠ y evaluar detalladamente el desempe√±o del modelo en clasificaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e2acc450",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, title=\"Confusion Matrix\"):\n",
    "    if y_true is None or y_pred is None:\n",
    "        return\n",
    "\n",
    "    # Asegurar que son arrays de numpy\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Etiqueta Real')\n",
    "    plt.xlabel('Etiqueta Predicha')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60bb93f",
   "metadata": {},
   "source": [
    "#### Guardar campos en el CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "68f5403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def save_results(all_results):\n",
    "    if not all_results:\n",
    "        print(\"No se realizaron experimentos para guardar.\")\n",
    "        return\n",
    "\n",
    "    # Convertir history_dict, y_true y y_pred a JSON string para guardarlo en CSV\n",
    "    df_results = pd.DataFrame(all_results)\n",
    "\n",
    "    df_results[\"history_dict\"] = df_results[\"history_dict\"].apply(json.dumps)\n",
    "    df_results[\"y_true\"] = df_results[\"y_true\"].apply(lambda x: json.dumps(x.tolist()))\n",
    "    df_results[\"y_pred\"] = df_results[\"y_pred\"].apply(lambda x: json.dumps(x.tolist()))\n",
    "\n",
    "    # Duplicado de df sin columnas internas para visualizaci√≥n\n",
    "    df_csv = df_results.T\n",
    "    csv_filename = \"resultados_cnn_fer2.csv\"\n",
    "\n",
    "    # Convertir decimales '.' ‚Üí ','\n",
    "    def decimal_to_comma(df):\n",
    "        return df.applymap(\n",
    "            lambda x: str(x).replace('.', ',') if isinstance(x, (float, int)) else x\n",
    "        )\n",
    "\n",
    "    # Guardar o a√±adir\n",
    "    if not os.path.exists(csv_filename):\n",
    "        df_to_save = decimal_to_comma(df_csv)\n",
    "        df_to_save.to_csv(csv_filename, sep=';', index=True)\n",
    "    else:\n",
    "        df_prev = pd.read_csv(csv_filename, sep=';', index_col=0)\n",
    "        df_new = decimal_to_comma(df_csv)\n",
    "        df_combined = pd.concat([df_prev, df_new], axis=0)\n",
    "        df_combined.to_csv(csv_filename, sep=';', index=True)\n",
    "\n",
    "    print(f\"Resultados guardados en: {csv_filename}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7af677b",
   "metadata": {},
   "source": [
    "#### Leer desde el CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "48dd046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_json_load(x):\n",
    "    \"\"\"\n",
    "    Intenta cargar JSON de forma estricta. Si falla, intenta corregir\n",
    "    formatos comunes (comillas simples, booleanos de Python, etc.)\n",
    "    usando ast.literal_eval.\n",
    "    \"\"\"\n",
    "    if not isinstance(x, str):\n",
    "        return x\n",
    "    try:\n",
    "        return json.loads(x)\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        try:\n",
    "            # Reemplazos para hacer compatible string formato Python con JSON/eval\n",
    "            x_fixed = x.replace(\"null\", \"None\").replace(\"true\", \"True\").replace(\"false\", \"False\")\n",
    "            return ast.literal_eval(x_fixed)\n",
    "        except Exception as e:\n",
    "            print(f\"Error decodificando: {e}\")\n",
    "            return None\n",
    "\n",
    "def load_all_experiments_from_csv(csv_filename):\n",
    "    # 1. Leer con el separador correcto (punto y coma seg√∫n tu archivo)\n",
    "    df = pd.read_csv(csv_filename, sep=';', index_col=0)\n",
    "    \n",
    "    # 2. Transponer (Filas = Experimentos)\n",
    "    df = df.T\n",
    "    \n",
    "    # 3. Convertir columnas JSON de forma robusta\n",
    "    json_cols = [\"history_dict\", \"y_true\", \"y_pred\"]\n",
    "    for col in json_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(robust_json_load)\n",
    "            \n",
    "    # 4. Convertir test_acc a float manejando la coma decimal\n",
    "    if \"test_acc\" in df.columns:\n",
    "        # Reemplaza ',' por '.' y convierte a float\n",
    "        df[\"test_acc\"] = df[\"test_acc\"].astype(str).str.replace(',', '.').astype(float)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c2b077",
   "metadata": {},
   "source": [
    "#### Analizar mejor y peor experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "67c880e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_from_csv(csv_filename, classes):\n",
    "    # Cargar datos\n",
    "    df = load_all_experiments_from_csv(csv_filename)\n",
    "\n",
    "    # Identificar mejor y peor experimento\n",
    "    best = df.loc[df[\"test_acc\"].idxmax()]\n",
    "    worst = df.loc[df[\"test_acc\"].idxmin()]\n",
    "\n",
    "    print(f\"üèÜ Mejor experimento: {best['name']} - {best['test_acc']}%\")\n",
    "    print(f\"üìâ Peor experimento: {worst['name']} - {worst['test_acc']}%\")\n",
    "\n",
    "    # Graficar Mejor\n",
    "    plot_training_history(best[\"history_dict\"], title=f\"Mejor experimento\")\n",
    "    plot_confusion_matrix(best[\"y_true\"], best[\"y_pred\"], classes, f\"Confusion ({best['name']})\")\n",
    "\n",
    "    # Graficar Peor\n",
    "    plot_training_history(worst[\"history_dict\"], title=f\"Peor experimento\")\n",
    "    plot_confusion_matrix(worst[\"y_true\"], worst[\"y_pred\"], classes, f\"Confusion ({worst['name']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d367eb26",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Entrenamiento CNN desde cero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e77965",
   "metadata": {},
   "source": [
    "#### Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ba943f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Rescaling, Input, LeakyReLU\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def build_modular_cnn(\n",
    "    input_shape=(48, 48, 1),\n",
    "    conv_layers=[(64,3,1), (128,3,1), (256,3,2)],\n",
    "    fc_layers=[256,128],\n",
    "    dropout_conv=0.3,\n",
    "    dropout_fc=0.4,\n",
    "    num_classes=6,\n",
    "    activation=LeakyReLU,   \n",
    "    lr=1e-3\n",
    "):\n",
    "    \"\"\"\n",
    "    Crea un modelo CNN modular tipo FER, basado en:\n",
    "    conv_layers = [(filters, kernel_size, repeats), ...]\n",
    "    fc_layers   = [n1, n2, ...]\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(Rescaling(1./255))\n",
    "\n",
    "    # ----- BLOQUES CONVOLUCIONALES -----\n",
    "    for filters, k, reps in conv_layers:\n",
    "        for _ in range(reps):\n",
    "            model.add(Conv2D(filters, (k, k), padding=\"same\"))\n",
    "            model.add(activation())   \n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Dropout(dropout_conv))\n",
    "\n",
    "    # ----- CAPAS DENSAS -----\n",
    "    model.add(Flatten())\n",
    "\n",
    "    for units in fc_layers:\n",
    "        model.add(Dense(units))\n",
    "        model.add(activation())\n",
    "        model.add(Dropout(dropout_fc))\n",
    "\n",
    "    # ----- SALIDA -----\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    # ----- COMPILAR -----\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=lr),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca95a9cd",
   "metadata": {},
   "source": [
    "#### Entrenar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "00fe5da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_ds,\n",
    "    validation_ds,\n",
    "    epochs=50,\n",
    "    patience=10,\n",
    "    monitor=\"val_accuracy\",\n",
    "    mode=\"max\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Entrena un modelo con EarlyStopping parametrizable.\n",
    "    \"\"\"\n",
    "\n",
    "    es = EarlyStopping(\n",
    "        monitor=monitor,\n",
    "        mode=mode,\n",
    "        patience=patience,\n",
    "        verbose=1,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_ds,\n",
    "        callbacks=[es]\n",
    "    )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a89adac",
   "metadata": {},
   "source": [
    "#### Evaluar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2163523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset):\n",
    "    results = model.evaluate(dataset, verbose=0)\n",
    "    loss = results[0]\n",
    "    acc = results[1] * 100 # Convertir a porcentaje\n",
    "\n",
    "    # 1. Obtener predicciones y etiquetas reales para la Matriz de Confusi√≥n\n",
    "    y_pred_probs = model.predict(dataset, verbose=0)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    # 2. Obtener etiquetas verdaderas (tf.data.Dataset requiere iteraci√≥n)\n",
    "    y_true = []\n",
    "    for _, labels in dataset.unbatch().as_numpy_iterator():\n",
    "        y_true.append(np.argmax(labels))\n",
    "    y_true = np.array(y_true)\n",
    "\n",
    "    return acc, loss, y_true, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854049f2",
   "metadata": {},
   "source": [
    "#### Entrenamientos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8214979a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando dataset (FER-2013)...\n",
      "Found 28219 files belonging to 6 classes.\n",
      "Found 3527 files belonging to 6 classes.\n",
      "Found 3527 files belonging to 6 classes.\n",
      "\n",
      "====================================\n",
      "Entrenando experimento: CNN_VGG_Deep_Best\n",
      "====================================\n",
      "\n",
      "Epoch 1/3\n",
      "\u001b[1m441/441\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 437ms/step - accuracy: 0.2544 - loss: 1.7610 - val_accuracy: 0.2515 - val_loss: 1.7341\n",
      "Epoch 2/3\n",
      "\u001b[1m441/441\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 364ms/step - accuracy: 0.3624 - loss: 1.5568 - val_accuracy: 0.4732 - val_loss: 1.3443\n",
      "Epoch 3/3\n",
      "\u001b[1m206/441\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m1:23\u001b[0m 357ms/step - accuracy: 0.4620 - loss: 1.3708"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "def main():\n",
    "    \"\"\"Funci√≥n principal optimizada para maximizar Accuracy en FER-2013,\n",
    "    con guardado completo en CSV y an√°lisis desde hist√≥rico.\"\"\"\n",
    "    \n",
    "    # --- Configuraci√≥n global ---\n",
    "    BATCH_SIZE = 64\n",
    "    INPUT_CHANNELS = 1\n",
    "    COLOR_MODE = 'grayscale' if INPUT_CHANNELS == 1 else 'rgb'\n",
    "\n",
    "    # DEFINICI√ìN DE EXPERIMENTOS (puedes a√±adir m√°s sin tocar nada m√°s)\n",
    "    EXPERIMENTS = [\n",
    "                {\n",
    "            # MODELO 1: VGG-Style Deep (El candidato a ganar)\n",
    "            # Estrategia: Bloques dobles de convoluci√≥n para extraer m√°s texturas antes de reducir tama√±o.\n",
    "            # Filtros hasta 512 para capturar micro-expresiones.\n",
    "            \"name\": \"CNN_VGG_Deep_Best\",\n",
    "            \"conv_layers\": [\n",
    "                (64, 3, 2),   # 2 capas de 64 filtros\n",
    "                (128, 3, 2),  # 2 capas de 128 filtros\n",
    "                (256, 3, 2),  # 2 capas de 256 filtros\n",
    "                (512, 3, 2)   # 2 capas de 512 filtros\n",
    "            ],\n",
    "            \"fc_layers\": [512, 256], \n",
    "            \"lr\": 1e-4,       # LR bajo para estabilidad en red profunda\n",
    "            \"dropout_conv\": 0.3,\n",
    "            \"dropout_fc\": 0.5, # Dropout alto para evitar overfitting\n",
    "            \"epochs\": 3      # M√°s √©pocas porque el LR es bajo (EarlyStopping cortar√° si es necesario)\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # 1. SETUP & DATOS\n",
    "    seed_everything(42)\n",
    "    print(\"Descargando dataset (FER-2013)...\")\n",
    "    dataset_path = \"C:\\\\Users\\\\laura\\\\Downloads\\\\datos\"\n",
    "\n",
    "    # Preparar datos\n",
    "    train_loader, val_loader, test_loader, classes = prepare_dataloaders(\n",
    "        dataset_path,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        img_size=(48, 48),\n",
    "        color_mode=COLOR_MODE\n",
    "    )\n",
    "\n",
    "    all_results = []\n",
    "    input_shape = (48, 48, INPUT_CHANNELS)\n",
    "    \n",
    "    # 2. EJECUCI√ìN DE EXPERIMENTOS\n",
    "    for exp in EXPERIMENTS:\n",
    "        print(\"\\n====================================\")\n",
    "        print(f\"Entrenando experimento: {exp['name']}\")\n",
    "        print(\"====================================\\n\")\n",
    "\n",
    "        # A. CONSTRUIR\n",
    "        model = build_modular_cnn(\n",
    "            input_shape=input_shape,\n",
    "            conv_layers=exp[\"conv_layers\"],\n",
    "            fc_layers=exp[\"fc_layers\"],\n",
    "            dropout_conv=exp.get(\"dropout_conv\", 0.3),\n",
    "            dropout_fc=exp.get(\"dropout_fc\", 0.4),\n",
    "            lr=exp[\"lr\"],\n",
    "            num_classes=len(classes)\n",
    "        )\n",
    "\n",
    "        # B. ENTRENAR\n",
    "        history = train_model(\n",
    "            model=model,\n",
    "            train_ds=train_loader,\n",
    "            validation_ds=val_loader,\n",
    "            epochs=exp[\"epochs\"],\n",
    "            patience=8 \n",
    "        )\n",
    "\n",
    "        # C. GUARDAR HISTORIAL\n",
    "        history_dict = {\n",
    "            \"train_loss\": history.history[\"loss\"],\n",
    "            \"train_acc\": [x * 100 for x in history.history[\"accuracy\"]],\n",
    "            \"val_loss\": history.history[\"val_loss\"],\n",
    "            \"val_acc\": [x * 100 for x in history.history[\"val_accuracy\"]]\n",
    "        }\n",
    "\n",
    "        # D. EVALUAR\n",
    "        test_acc, test_loss, y_true, y_pred = evaluate(model, test_loader)\n",
    "        print(f\"[{exp['name']}] Test Accuracy = {test_acc:.4f}%\")\n",
    "\n",
    "        # E. ALMACENAR RESULTADOS (interno)\n",
    "        result_entry = {\n",
    "            \"name\": exp['name'],\n",
    "            \"conv_layers_config\": str(exp[\"conv_layers\"]),\n",
    "            \"fc_layers_config\": str(exp[\"fc_layers\"]),\n",
    "            \"learning_rate\": exp[\"lr\"],\n",
    "            \"dropout_conv\": exp.get(\"dropout_conv\", 0.3),\n",
    "            \"dropout_fc\": exp.get(\"dropout_fc\", 0.4),\n",
    "            \"epochs_run\": len(history.history[\"loss\"]),\n",
    "            \"test_loss\": test_loss,\n",
    "            \"test_acc\": test_acc,\n",
    "            \"history_dict\": history_dict,\n",
    "            \"y_true\": y_true,\n",
    "            \"y_pred\": y_pred\n",
    "        }\n",
    "        all_results.append(result_entry)\n",
    "\n",
    "    # 3. GUARDAR EN CSV (incluye decimales con coma)\n",
    "    save_results(all_results)\n",
    "\n",
    "    # 4. ANALIZAR HIST√ìRICO COMPLETO DESDE EL CSV\n",
    "    print(\"\\n===============================\")\n",
    "    print(\"üîé Analizando hist√≥rico completo desde CSV...\")\n",
    "    print(\"===============================\\n\")\n",
    "\n",
    "    analyze_from_csv(\"resultados_cnn_fer2.csv\", classes)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FSI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
