{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9072223d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laura\\anaconda3\\envs\\FSI\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dataset en: C:\\Users\\laura\\.cache\\kagglehub\\datasets\\pankaj4321\\fer-2013-facial-expression-dataset\\versions\\1\n",
      "Clases detectadas en train_dataset: ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
      "Número de clases: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laura\\anaconda3\\envs\\FSI\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\laura\\anaconda3\\envs\\FSI\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 1.5524, Train Accuracy: 41.14%, Val Loss: 1.3102, Val Accuracy: 50.71%, Time: 111.52s\n",
      "Epoch [2/20], Train Loss: 1.2222, Train Accuracy: 53.92%, Val Loss: 1.1224, Val Accuracy: 58.40%, Time: 105.69s\n",
      "Epoch [3/20], Train Loss: 1.0436, Train Accuracy: 60.96%, Val Loss: 1.0891, Val Accuracy: 59.35%, Time: 176.49s\n",
      "Epoch [4/20], Train Loss: 0.8612, Train Accuracy: 67.95%, Val Loss: 1.0988, Val Accuracy: 60.24%, Time: 140.63s\n",
      "Epoch [5/20], Train Loss: 0.7152, Train Accuracy: 73.54%, Val Loss: 1.1456, Val Accuracy: 60.99%, Time: 111.36s\n",
      "Epoch [6/20], Train Loss: 0.5880, Train Accuracy: 78.49%, Val Loss: 1.2017, Val Accuracy: 61.80%, Time: 101.73s\n",
      "Epoch [7/20], Train Loss: 0.4576, Train Accuracy: 83.57%, Val Loss: 1.3689, Val Accuracy: 58.54%, Time: 100.17s\n",
      "Epoch [8/20], Train Loss: 0.3703, Train Accuracy: 86.71%, Val Loss: 1.4569, Val Accuracy: 62.22%, Time: 90.78s\n",
      "Epoch [9/20], Train Loss: 0.2929, Train Accuracy: 89.64%, Val Loss: 1.5751, Val Accuracy: 60.35%, Time: 93.16s\n",
      "Epoch [10/20], Train Loss: 0.2463, Train Accuracy: 91.45%, Val Loss: 1.7216, Val Accuracy: 59.54%, Time: 100.18s\n",
      "Epoch [11/20], Train Loss: 0.2044, Train Accuracy: 92.76%, Val Loss: 1.7842, Val Accuracy: 60.91%, Time: 102.88s\n",
      "Epoch [12/20], Train Loss: 0.1834, Train Accuracy: 93.54%, Val Loss: 1.7977, Val Accuracy: 61.35%, Time: 223.20s\n",
      "Epoch [13/20], Train Loss: 0.1607, Train Accuracy: 94.39%, Val Loss: 1.7264, Val Accuracy: 61.19%, Time: 87.60s\n",
      "Epoch [14/20], Train Loss: 0.1492, Train Accuracy: 94.84%, Val Loss: 1.8286, Val Accuracy: 62.27%, Time: 52.93s\n",
      "Epoch [15/20], Train Loss: 0.1348, Train Accuracy: 95.36%, Val Loss: 1.8819, Val Accuracy: 62.55%, Time: 117.87s\n",
      "Epoch [16/20], Train Loss: 0.1238, Train Accuracy: 95.76%, Val Loss: 1.9255, Val Accuracy: 61.08%, Time: 235.04s\n",
      "Epoch [17/20], Train Loss: 0.1198, Train Accuracy: 95.88%, Val Loss: 2.0025, Val Accuracy: 61.86%, Time: 276.51s\n",
      "Epoch [18/20], Train Loss: 0.1168, Train Accuracy: 95.87%, Val Loss: 1.9730, Val Accuracy: 61.49%, Time: 238.70s\n",
      "Epoch [19/20], Train Loss: 0.1021, Train Accuracy: 96.54%, Val Loss: 2.1108, Val Accuracy: 61.69%, Time: 238.97s\n",
      "Epoch [20/20], Train Loss: 0.1072, Train Accuracy: 96.24%, Val Loss: 2.0170, Val Accuracy: 62.19%, Time: 113.24s\n",
      "Test Loss: 1.8661, Test Accuracy: 63.33%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "import kagglehub\n",
    "\n",
    "#Directorio y parámetros iniciales\n",
    "DATA_DIR = kagglehub.dataset_download(\"pankaj4321/fer-2013-facial-expression-dataset\")\n",
    "print(\"Usando dataset en:\", DATA_DIR)\n",
    "BATCH_SIZE = 64\n",
    "IMG_SIZE = (48, 48)\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.0004\n",
    "SEED = 42\n",
    "\n",
    "#En caso de tener GPU no usar CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(SEED) #Generación por semilla\n",
    "\n",
    "#Transformación del formato de imágenes\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "#Cargar datos de entrenamiento, validación y prueba\n",
    "train_dataset = datasets.ImageFolder(root=os.path.join(DATA_DIR, 'train'), transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=os.path.join(DATA_DIR, 'val'), transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=os.path.join(DATA_DIR, 'test'), transform=transform)\n",
    "\n",
    "print(\"Clases detectadas en train_dataset:\", train_dataset.classes)\n",
    "print(\"Número de clases:\", len(train_dataset.classes))\n",
    "\n",
    "\n",
    "#Carga de datos por lotes\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "#Transfer learning con resnet18\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, len(train_dataset.classes))\n",
    "model = model.to(device)\n",
    "\n",
    "#Función de pérdida y optimizador\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999), eps=1e-08)\n",
    "scheduler = LinearLR(optimizer, start_factor=0.1, total_iters=int(EPOCHS * 0.1)) #Reduce el learning rate por épocas\n",
    "\n",
    "#Listas para almacenar las métricas\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
    "\n",
    "#Función evaluación del modelo\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    loss = running_loss / len(loader)\n",
    "    return loss, accuracy, all_labels, all_preds\n",
    "\n",
    "#Entrenamiento\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    epoch_start_time = time.time() #Tiempo de ejecución por época\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    val_loss, val_accuracy, _, _ = evaluate(model, val_loader, criterion)\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_accuracy)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    scheduler.step() #cambiar learning rate\n",
    "\n",
    "    #Estadísticas de época\n",
    "    print(f\"Epoch [{epoch + 1}/{EPOCHS}], Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.2f}%, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%, Time: {epoch_time:.2f}s\")\n",
    "\n",
    "#Evaluación de conjunto de prueba\n",
    "test_loss, test_accuracy, test_labels, test_preds = evaluate(model, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3090443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "import kagglehub\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURACIÓN INICIAL\n",
    "# ==========================================\n",
    "\n",
    "# Configuración de Semilla para reproducibilidad\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Descarga del dataset\n",
    "DATA_DIR = kagglehub.dataset_download(\"pankaj4321/fer-2013-facial-expression-dataset\")\n",
    "print(\"Usando dataset en:\", DATA_DIR)\n",
    "\n",
    "# Parámetros\n",
    "BATCH_SIZE = 64\n",
    "IMG_SIZE = (48, 48)\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.0004\n",
    "\n",
    "# Detección de hardware (Keras lo hace automático, pero lo imprimimos)\n",
    "print(\"Dispositivos disponibles:\", tf.config.list_physical_devices())\n",
    "\n",
    "# ==========================================\n",
    "# 2. CARGA DE DATOS (Data Pipeline)\n",
    "# ==========================================\n",
    "\n",
    "# Función auxiliar para cargar datasets\n",
    "def load_dataset(directory, subset_name):\n",
    "    full_path = os.path.join(DATA_DIR, directory)\n",
    "    if not os.path.exists(full_path):\n",
    "        print(f\"Advertencia: No se encontró {full_path}, buscando rutas alternativas...\")\n",
    "        # Ajuste por si la estructura de carpetas difiere\n",
    "        full_path = DATA_DIR if subset_name != 'val' else os.path.join(DATA_DIR, 'test')\n",
    "\n",
    "    return tf.keras.utils.image_dataset_from_directory(\n",
    "        full_path,\n",
    "        labels='inferred',\n",
    "        label_mode='categorical', # Para CrossEntropy\n",
    "        class_names=None,\n",
    "        color_mode='rgb',         # Equivalente a transformar a 3 canales\n",
    "        batch_size=BATCH_SIZE,\n",
    "        image_size=IMG_SIZE,\n",
    "        shuffle=(subset_name == 'train'),\n",
    "        seed=SEED if subset_name == 'train' else None\n",
    "    )\n",
    "\n",
    "print(\"Cargando datos...\")\n",
    "train_ds = load_dataset('train', 'train')\n",
    "val_ds  = load_dataset('val', 'val') # Nota: A veces FER2013 en Kaggle llama a val 'test'\n",
    "test_ds = load_dataset('test', 'test')\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(\"Clases detectadas:\", class_names)\n",
    "print(\"Número de clases:\", len(class_names))\n",
    "\n",
    "# Optimización de carga (Prefetching) para velocidad\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# ==========================================\n",
    "# 3. DEFINICIÓN DEL MODELO (Transfer Learning)\n",
    "# ==========================================\n",
    "\n",
    "def build_model(num_classes):\n",
    "    inputs = tf.keras.Input(shape=(48, 48, 3))\n",
    "    \n",
    "    # 1. Normalización equivalente a PyTorch transforms.Normalize((0.5), (0.5))\n",
    "    # Entrada [0, 255] -> Salida [-1, 1]\n",
    "    x = layers.Rescaling(scale=1./127.5, offset=-1)(inputs)\n",
    "    \n",
    "    # 2. Base Pre-entrenada (ResNet50V2 es la alternativa estándar a ResNet18 en Keras)\n",
    "    # include_top=False elimina la capa densa final de ImageNet\n",
    "    base_model = tf.keras.applications.ResNet50V2(\n",
    "        include_top=False, \n",
    "        weights='imagenet', \n",
    "        input_tensor=x\n",
    "    )\n",
    "    \n",
    "    # En PyTorch model.train() entrena todo por defecto. Aquí hacemos lo mismo:\n",
    "    base_model.trainable = True \n",
    "    \n",
    "    # 3. Cabezal de clasificación (Top layers)\n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x) # Convierte features 2D a vector 1D\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name=\"ResNet_Transfer\")\n",
    "    return model\n",
    "\n",
    "model = build_model(len(class_names))\n",
    "\n",
    "# ==========================================\n",
    "# 4. COMPILACIÓN Y SCHEDULER\n",
    "# ==========================================\n",
    "\n",
    "# Optimizador equivalente\n",
    "optimizer = optimizers.Adam(\n",
    "    learning_rate=LEARNING_RATE, \n",
    "    beta_1=0.9, \n",
    "    beta_2=0.999, \n",
    "    epsilon=1e-08\n",
    ")\n",
    "\n",
    "# Función de pérdida\n",
    "loss_fn = 'categorical_crossentropy'\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# Scheduler: Simulación de LinearLR (Warmup o Decay lineal)\n",
    "# Tu código PyTorch usaba LinearLR con total_iters = 10% de las épocas.\n",
    "# Aquí definimos un scheduler simple que reduce el LR linealmente o lo mantiene.\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < int(EPOCHS * 0.1):\n",
    "        return lr # Mantener o hacer warmup (aquí mantenemos para simplificar)\n",
    "    return lr # O aplicar decay: lr * 0.95\n",
    "\n",
    "scheduler_cb = callbacks.LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "# ==========================================\n",
    "# 5. ENTRENAMIENTO\n",
    "# ==========================================\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[scheduler_cb],\n",
    "    verbose=1 \n",
    ")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Tiempo total de entrenamiento: {total_time:.2f}s\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. EVALUACIÓN\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\nEvaluando en conjunto de prueba...\")\n",
    "test_loss, test_acc = model.evaluate(test_ds)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "# Si necesitas las predicciones y etiquetas (como en tu función PyTorch)\n",
    "# y_pred = model.predict(test_ds)\n",
    "# y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "# y_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "# y_true_classes = np.argmax(y_true, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FSI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
