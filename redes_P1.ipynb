{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4833f3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # PyTorch\n",
    "from torchvision import datasets, transforms # Datasets y Transformaciones (Para la parte de datos)\n",
    "from torch.utils.data import DataLoader # Para cargar los datos en la red\n",
    "from torch import nn, optim # Para crear la red y el optimizador\n",
    "import torch.nn.functional as F # Funciones de activación y pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "142390fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración\n",
    "BATCH_SIZE = 64\n",
    "DATA_DIR = \"./data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48000841",
   "metadata": {},
   "source": [
    "### Preparación de datos\n",
    "El primer paso es preparar los datos que vamos a utilizar para entrenar la red. Esto incluye cargar los datos, aplicar transformaciones necesarias (como normalización, aumento de datos, etc.) y dividirlos en conjuntos de entrenamiento y test. Suele ser comun dividir los datos en un 80% para entrenamiento y un 20% para test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f918f6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones útiles\n",
    "def get_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "def get_dataset(train=True, transform=None):\n",
    "    return datasets.MNIST(\n",
    "        root=DATA_DIR,\n",
    "        train=train,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "def get_dataloader(dataset, batch_size=BATCH_SIZE, shuffle=True):\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72896de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar los datos\n",
    "def load_mnist(batch_size=BATCH_SIZE):\n",
    "    transform = get_transform()\n",
    "    train_dataset = get_dataset(train=True, transform=transform)\n",
    "    test_dataset = get_dataset(train=False, transform=transform)\n",
    "\n",
    "    train_loader = get_dataloader(train_dataset, batch_size, shuffle=True)\n",
    "    test_loader = get_dataloader(test_dataset, batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader, train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99a6a64",
   "metadata": {},
   "source": [
    "#### Creación de la red\n",
    "\n",
    "A continuación, definimos la arquitectura de la red neuronal. Esto implica decidir el número de capas, el tipo de capas (convolucionales, lineales, etc.), las funciones de activación y cómo se conectan entre sí.\n",
    "\n",
    "Para definir una red neuronal en PyTorch, se crea una clase que hereda de `nn.Module`. Dentro de esta clase, se definen las capas de la red en el método `__init__` y la lógica de cómo los datos fluyen a través de estas capas en el método `forward`. Aquí hay un ejemplo simple de una red neuronal con dos capas completamente conectadas (fully connected):\n",
    "\n",
    "Las fully connected layers (o capas densas) son aquellas en las que cada neurona de una capa está conectada a todas las neuronas de la capa siguiente. Esto permite que la red aprenda representaciones complejas de los datos, ya que cada neurona puede combinar información de todas las neuronas anteriores.\n",
    "\n",
    "nn.Linear(in_features, out_features) es una capa lineal que aplica una transformación lineal a los datos de entrada. Aquí, in_features es el número de características de entrada (por ejemplo, el tamaño del vector de entrada) y out_features es el número de características de salida (por ejemplo, el número de neuronas en la capa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8ede6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class ModularNN(nn.Module):\n",
    "    def __init__(self, input_size=28*28, hidden_layers=[128, 64], output_size=10, activation=nn.ReLU):\n",
    "        \"\"\"\n",
    "        input_size: tamaño de entrada (por defecto 28*28 para imágenes MNIST)\n",
    "        hidden_layers: lista con el número de neuronas por cada capa oculta\n",
    "        output_size: número de clases de salida\n",
    "        activation: clase de función de activación (nn.ReLU, nn.Tanh, nn.Sigmoid, etc.)\n",
    "        \"\"\"\n",
    "        super(ModularNN, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        prev_size = input_size\n",
    "        \n",
    "        # Crear las capas ocultas\n",
    "        for hidden_size in hidden_layers:\n",
    "            self.layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            self.layers.append(activation())  # Función de activación\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Capa de salida\n",
    "        self.layers.append(nn.Linear(prev_size, output_size))\n",
    "        self.layers.append(nn.Softmax(dim=1))\n",
    "        \n",
    "        # Convertir la lista de capas en un Sequential\n",
    "        self.network = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Aplanar la entrada\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b37a1fe",
   "metadata": {},
   "source": [
    "```python \n",
    "# Red con 2 capas ocultas de 128 y 64 neuronas\n",
    "model = ModularNN(hidden_layers=[128, 64])\n",
    "\n",
    "# Red con 3 capas ocultas de 256, 128 y 64 neuronas\n",
    "model2 = ModularNN(hidden_layers=[256, 128, 64], activation=nn.Tanh)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a8e0f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "def summary(model, input_size):\n",
    "    sum = summary(model, input_size)\n",
    "    print(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267fa32c",
   "metadata": {},
   "source": [
    "### Optimizador\n",
    "\n",
    "La función de pérdida mide qué tan bien está funcionando la red. El optimizador ajusta los pesos de la red para minimizar la pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6adb0272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_function(loss_name='MSELoss', **kwargs):\n",
    "    \"\"\"\n",
    "    Devuelve la función de pérdida configurada.\n",
    "    \n",
    "    loss_name: 'MSELoss', 'CrossEntropyLoss', 'L1Loss', etc.\n",
    "    kwargs: otros parámetros específicos de la función de pérdida\n",
    "    \"\"\"\n",
    "    if loss_name == 'MSELoss':\n",
    "        return nn.MSELoss(**kwargs)\n",
    "    elif loss_name == 'CrossEntropyLoss':\n",
    "        return nn.CrossEntropyLoss(**kwargs)\n",
    "    elif loss_name == 'L1Loss':\n",
    "        return nn.L1Loss(**kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Función de pérdida '{loss_name}' no soportada\")\n",
    "\n",
    "def get_optimizer(model, optimizer_name='SGD', lr=0.01, **kwargs):\n",
    "    if optimizer_name == 'SGD':\n",
    "        return optim.SGD(model.parameters(), lr=lr, **kwargs)\n",
    "    elif optimizer_name == 'Adam':\n",
    "        return optim.Adam(model.parameters(), lr=lr, **kwargs)\n",
    "    elif optimizer_name == 'RMSprop':\n",
    "        return optim.RMSprop(model.parameters(), lr=lr, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizador '{optimizer_name}' no soportado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca89728",
   "metadata": {},
   "source": [
    "```python\n",
    "# Loss function\n",
    "criterion = get_loss_function(loss_name='CrossEntropyLoss')\n",
    "\n",
    "# Optimizer\n",
    "optimizer = get_optimizer(model, optimizer_name='Adam', lr=0.001)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de9a607",
   "metadata": {},
   "source": [
    "### Entrenamiento de la red\n",
    "\n",
    "En este paso, alimentamos los datos de entrenamiento a la red, calculamos la pérdida, y actualizamos los pesos utilizando el optimizador. Este proceso se repite durante varias épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10b110b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, device=torch.device(\"cpu\"), epochs=5, one_hot=False, num_classes=10, verbose=True):\n",
    "    \"\"\"\n",
    "    Función modular para entrenar un modelo PyTorch.\n",
    "    \n",
    "    model: nn.Module\n",
    "    train_loader: DataLoader con los datos de entrenamiento\n",
    "    criterion: función de pérdida\n",
    "    optimizer: optimizador\n",
    "    device: 'cpu' o 'cuda'\n",
    "    epochs: número de épocas de entrenamiento\n",
    "    one_hot: si True convierte las etiquetas a one-hot (útil para MSELoss)\n",
    "    num_classes: número de clases (para one-hot)\n",
    "    verbose: si True imprime la pérdida por época\n",
    "    \"\"\"\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if one_hot:\n",
    "                labels = F.one_hot(labels, num_classes=num_classes).float()\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        if verbose:\n",
    "            avg_loss = running_loss / len(train_loader)\n",
    "            print(f'[Epoch {epoch + 1}] loss: {avg_loss:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060dd2be",
   "metadata": {},
   "source": [
    "### Evaluación de la red"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6849b73",
   "metadata": {},
   "source": [
    "Para evaluar el rendimiento de la red, se utiliza el conjunto de test. Esto implica pasar los datos de test a través de la red y comparar las predicciones con las etiquetas reales para calcular métricas como la precisión (accuracy). El accuracy es la proporción de predicciones correctas sobre el total de predicciones realizadas. Se calcula como:\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Número de predicciones correctas}}{\\text{Número total de predicciones}} \\times 100\\%\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccca5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()  # Poner el modelo en modo evaluación\n",
    "    correct = 0\n",
    "    total = test_loader.dataset.__len__()  # Total de muestras en el conjunto de test\n",
    "    print(f'Total de muestras en el conjunto de test: {total}')\n",
    "    with torch.no_grad():  # No calcular gradientes\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)  # Mover datos al dispositivo\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Obtener las predicciones\n",
    "            correct += (predicted == labels).sum().item()  # Actualizar el contador de aciertos\n",
    "    accuracy = 100 * correct / total if total > 0 else 0\n",
    "    print(f'Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3ee8e8",
   "metadata": {},
   "source": [
    "# Ejercicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b7af61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Experimento: {'batch_size': 64, 'hidden_layers': [256, 128], 'activation': <class 'torch.nn.modules.activation.ReLU'>, 'loss': 'MSELoss', 'optimizer': 'Adam', 'lr': 0.001} ===\n",
      "[Epoch 1] loss: 0.016\n",
      "[Epoch 2] loss: 0.008\n",
      "[Epoch 3] loss: 0.006\n",
      "[Epoch 4] loss: 0.005\n",
      "[Epoch 5] loss: 0.005\n",
      "Total de muestras en el conjunto de test: 10000\n",
      "Accuracy: 96.76%\n",
      "\n",
      "=== Experimento: {'batch_size': 64, 'hidden_layers': [512, 256], 'activation': <class 'torch.nn.modules.activation.ReLU'>, 'loss': 'MSELoss', 'optimizer': 'Adam', 'lr': 0.001} ===\n",
      "[Epoch 1] loss: 0.015\n",
      "[Epoch 2] loss: 0.008\n",
      "[Epoch 3] loss: 0.006\n",
      "[Epoch 4] loss: 0.005\n",
      "[Epoch 5] loss: 0.004\n",
      "Total de muestras en el conjunto de test: 10000\n",
      "Accuracy: 96.49%\n",
      "\n",
      "=== Experimento: {'batch_size': 64, 'hidden_layers': [256, 128], 'activation': <class 'torch.nn.modules.activation.ReLU'>, 'loss': 'MSELoss', 'optimizer': 'Adam', 'lr': 0.001} ===\n",
      "[Epoch 1] loss: 0.016\n",
      "[Epoch 2] loss: 0.008\n",
      "[Epoch 3] loss: 0.006\n",
      "[Epoch 4] loss: 0.005\n",
      "[Epoch 5] loss: 0.005\n",
      "Total de muestras en el conjunto de test: 10000\n",
      "Accuracy: 96.80%\n",
      "\n",
      "=== Experimento: {'batch_size': 64, 'hidden_layers': [512, 256], 'activation': <class 'torch.nn.modules.activation.ReLU'>, 'loss': 'MSELoss', 'optimizer': 'Adam', 'lr': 0.001} ===\n",
      "[Epoch 1] loss: 0.015\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m optimizer = get_optimizer(model, optimizer_name=exp[\u001b[33m\"\u001b[39m\u001b[33moptimizer\u001b[39m\u001b[33m\"\u001b[39m], lr=exp[\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     24\u001b[39m criterion = get_loss_function(\u001b[33m'\u001b[39m\u001b[33mMSELoss\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# Puedes probar CrossEntropyLoss también\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_hot\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m evaluate(model, test_loader)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, criterion, optimizer, device, epochs, one_hot, num_classes, verbose)\u001b[39m\n\u001b[32m     32\u001b[39m     labels = F.one_hot(labels, num_classes=num_classes).float()\n\u001b[32m     34\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m optimizer.step()\n\u001b[32m     38\u001b[39m running_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Laura\\anaconda3\\envs\\FSI\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Laura\\anaconda3\\envs\\FSI\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Laura\\anaconda3\\envs\\FSI\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "experiments = [\n",
    "    {\"hidden_layers\": [128], \"activation\": nn.ReLU, \"optimizer\": \"SGD\", \"lr\": 0.01, \"batch_size\": 32},\n",
    "    {\"hidden_layers\": [256, 128], \"activation\": nn.ReLU, \"optimizer\": \"Adam\", \"lr\": 0.001, \"batch_size\": 64},\n",
    "    {\"hidden_layers\": [128, 64, 32], \"activation\": nn.Tanh, \"optimizer\": \"RMSprop\", \"lr\": 0.0005, \"batch_size\": 128},\n",
    "    {\"hidden_layers\": [512, 256], \"activation\": nn.Sigmoid, \"optimizer\": \"SGD\", \"lr\": 0.1, \"batch_size\": 32},\n",
    "]\n",
    "\n",
    "for exp in experiments:\n",
    "    print(f\"\\n=== Experimento: {exp} ===\")\n",
    "    train_loader, test_loader, _ = load_mnist(batch_size=exp[\"batch_size\"])\n",
    "    \n",
    "    model = ModularNN(hidden_layers=exp[\"hidden_layers\"], activation=exp[\"activation\"])\n",
    "    optimizer = get_optimizer(model, optimizer_name=exp[\"optimizer\"], lr=exp[\"lr\"])\n",
    "    criterion = get_loss_function('MSELoss')  # Puedes probar CrossEntropyLoss también\n",
    "    \n",
    "    train_model(model, train_loader, criterion, optimizer, device=DEVICE, epochs=5, one_hot=True)\n",
    "    evaluate(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FSI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
